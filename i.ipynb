{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ast\n",
    "\n",
    "# # Load the Excel file\n",
    "# df = pd.read_excel('car_details_by_citys.xlsx')\n",
    "\n",
    "# # Function to safely convert strings to dictionaries\n",
    "# def to_dict_safe(value):\n",
    "#     try:\n",
    "#         # Convert only if the value is a string that represents a dictionary\n",
    "#         return ast.literal_eval(value) if isinstance(value, str) else value\n",
    "#     except (ValueError, SyntaxError):\n",
    "#         return {}\n",
    "\n",
    "# # Apply the function to each JSON-like column\n",
    "# df['new_car_detail'] = df['new_car_detail'].apply(to_dict_safe)\n",
    "# df['new_car_overview'] = df['new_car_overview'].apply(to_dict_safe)\n",
    "# df['new_car_feature'] = df['new_car_feature'].apply(to_dict_safe)\n",
    "# df['new_car_specs'] = df['new_car_specs'].apply(to_dict_safe)\n",
    "\n",
    "# # Normalize each dictionary column\n",
    "# car_detail_df = pd.json_normalize(df['new_car_detail'])\n",
    "# car_overview_df = pd.json_normalize(df['new_car_overview'], record_path='top', meta=['heading'], errors='ignore')\n",
    "# car_feature_df = pd.json_normalize(df['new_car_feature'], record_path='top', meta=['heading'], errors='ignore')\n",
    "# car_specs_df = pd.json_normalize(df['new_car_specs'], record_path='top', meta=['heading'], errors='ignore')\n",
    "\n",
    "# # Add columns with 'city' and 'car_links' to the main DataFrame before concatenation\n",
    "# df_main = df[['city', 'car_links']]\n",
    "\n",
    "# # Combine all normalized DataFrames\n",
    "# final_df = pd.concat([df_main, car_detail_df, car_overview_df, car_feature_df, car_specs_df], axis=1)\n",
    "\n",
    "# # Save the combined DataFrame to a new Excel file\n",
    "# final_df.to_excel('extracted_car_details.xlsx', index=False)\n",
    "\n",
    "# print(\"Data extracted and saved successfully as 'extracted_car_details.xlsx'\")\n",
    "\n",
    "# car_details  = pd.read_excel('extracted_car_details.xlsx')\n",
    "# car_details \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car_details_cleaned['No of Cylinder'].unique()\n",
    "\n",
    "# car_details_cleaned[['Turbo Charger', 'Super Charger' ]]\n",
    "\n",
    "# car_details_cleaned[['Turbo Charger', 'Super Charger','Alloy Wheel Size', 'Wheel Size' ]].isnull().sum()\n",
    "\n",
    "# # car_details_cleaned['Turbo Charger', 'Super Charger','modelYear','Registration Year','Year of Manufacture'].nunique().sum()\n",
    "\n",
    "# # Select multiple columns using double square brackets\n",
    "# unique_values_sum = car_details_cleaned[['Turbo Charger', 'Super Charger','Engine', 'Alloy Wheel Size', 'Wheel Size' ]].nunique()\n",
    "\n",
    "# unique_values_sum\n",
    "\n",
    "# # Compare the columns\n",
    "# car_details_cleaned['Comparisons'] = car_details_cleaned['Alloy Wheel Size'] == car_details_cleaned['Wheel Size']\n",
    "# car_details_cleaned\n",
    "\n",
    "# car_details_cleaned['Comparisons'].value_counts()\n",
    "\n",
    "# # Print rows where 'Comparisons' column is False\n",
    "# false_rows = car_details_cleaned[car_details_cleaned['Comparisons'] == False]\n",
    "# false_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Sample columns to remove units from\n",
    "# columns_to_clean = ['Column1', 'Column2', 'Column3', ...]  # replace with your actual column names\n",
    "\n",
    "# # Define a regex pattern to match all units you want to remove\n",
    "# pattern = r'\\b(lit|kg|km|mm|lakh|₹)\\b'\n",
    "\n",
    "# # Apply the regex pattern to each specified column\n",
    "# for column in columns_to_clean:\n",
    "#     car_details_cleaned[column] = car_details_cleaned[column].astype(str).str.replace(pattern, '', regex=True)\n",
    "\n",
    "# # Optionally, you can strip whitespace if needed\n",
    "# car_details_cleaned[columns_to_clean] = car_details_cleaned[columns_to_clean].apply(lambda x: x.str.strip())\n",
    "\n",
    "# print(car_details_cleaned[columns_to_clean])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the columns to clean\n",
    "columns_to_clean = ['Column1', 'Column2', 'Column3', ...]  # replace with your actual column names\n",
    "\n",
    "# Define the regex pattern to match all units you want to remove\n",
    "pattern = r'\\b(₹|Lakh|kmpl|CC|mm|kg|metres|Kmph|Sec|Seconds|Lit|litres)\\b'\n",
    "\n",
    "# Apply the regex pattern to each specified column where dtype is object\n",
    "for column in columns_to_clean:\n",
    "    if Final[column].dtype == 'object':\n",
    "        Final[column] = Final[column].str.replace(pattern, '', regex=True).str.strip()\n",
    "\n",
    "print(Final[columns_to_clean].head())  # Display cleaned columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final['price'].unique()\n",
    "Final['price'].value_counts()\n",
    "Final['Compression Ratio'].unique()\n",
    "Final['BoreX Stroke'].unique()\n",
    "Final['Gross Weight'].unique()\n",
    "Final['Ground Clearance Unladen'].unique()\n",
    "Final['Kms_Driven'].unique()\n",
    "Final['Engine_Displacement'].unique()\n",
    "Final['Alloy Wheel Size'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter for columns with dtype 'object' and display their names and values\n",
    "# for col in Final.select_dtypes(include='object').columns:\n",
    "#     print(f\"Column: {col}\")\n",
    "#     print(Final[col].head())  # Print the first few rows for a preview of values\n",
    "#     print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Filter for columns with dtype 'object' and display their names and values\n",
    "# for col in Final.select_dtypes(include='object').columns:\n",
    "    # display(Final[col].head())  # Display the first few rows for a preview of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# # Function to identify columns with unit values\n",
    "# def find_columns_with_units(Final):\n",
    "#     # Updated pattern to avoid capturing groups\n",
    "#     unit_pattern = re.compile(r'\\d+\\s?(?:km|Kmph|seconds|litres|kmph|km/hr|Kmph|mm|inch|kg|hp|bhp|lakhs|bhp|rpm|CC|₹|,|(PS)|)', re.IGNORECASE)\n",
    "#     columns_with_units = []\n",
    "#     for col in Final.columns:\n",
    "#         if Final[col].dtype == 'object':  # Check only object (string) columns\n",
    "#             # Use regex without capturing groups\n",
    "#             if Final[col].str.contains(unit_pattern, regex=True).any():\n",
    "#                 columns_with_units.append(col)\n",
    "#     return columns_with_units\n",
    "# # Identify columns with unit values\n",
    "# columns_with_units = find_columns_with_units(Final)\n",
    "# print(\"Columns with unit values:\", columns_with_units)\n",
    "\n",
    "# import re\n",
    "# import numpy as np\n",
    "# unit_pattern = re.compile(r'\\d+\\s?(?:km|Kmph|seconds|litres|kmph|km/hr|Kmph|mm|inch|kg|hp|bhp|Lakh|bhp|rpm|CC|₹|,|PS)', re.IGNORECASE)\n",
    "# # Separate numerical and categorical columns\n",
    "# numerical_cols = Final.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "# categorical_cols = Final.select_dtypes(include=['object']).columns.tolist()\n",
    "# # Function to remove units from the specified columns\n",
    "# def find_and_clean_columns_with_units(Finalf):\n",
    "#     columns_with_units = []\n",
    "#     for col in Final.columns:\n",
    "#         if Final[col].dtype == 'object':  # Check only object (string) columns\n",
    "#             if Final[col].str.contains(unit_pattern, regex=True).any():\n",
    "#                 # Remove the units and convert to numeric\n",
    "#                 # Remove everything except digits and a single decimal point\n",
    "#                 Final[col] = Final[col].str.extract(r'(\\d+\\.\\d+|\\d+)')[0].replace('', np.nan).astype(float)\n",
    "#                 columns_with_units.append(col)\n",
    "#     return columns_with_units\n",
    "# # Apply the function to identify and clean the columns\n",
    "# columns_with_units = find_and_clean_columns_with_units(Final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regex pattern to match all units you want to remove\n",
    "pattern = r'\\b(₹|,|Lakh|kmpl|CC|mm|kg|metres|Kmph|Sec|Seconds|Lit|litres)\\b'\n",
    "\n",
    "# Select only object-type columns and apply the regex replacement\n",
    "for i in Final.select_dtypes(include='object').columns:\n",
    "    Final[i] = Final[i].str.replace(pattern, '', regex=True).str.strip()\n",
    "\n",
    "# Display the first few rows of the cleaned DataFrame to confirm changes\n",
    "Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_value_imputer(df):\n",
    "    # Handling null values\n",
    "    for i in df.columns:\n",
    "        if (df[i].isnull().sum() / len(df)) * 100 >= 80:\n",
    "            df.drop(i, axis=1, inplace=True)\n",
    "        elif df[i].dtypes in [\"int64\", \"float64\"]:\n",
    "            if df[i].skew() < 0.4 and df[i].skew() > -0.4:\n",
    "                df[i].fillna(df[i].mean(), inplace=True)\n",
    "            else:\n",
    "                df[i].fillna(df[i].median(), inplace=True)\n",
    "        else:\n",
    "            df[i].fillna(df[i].mode()[0], inplace=True)\n",
    "\n",
    "    print(\"Null values after imputation:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # containing only numeric and categorical columns, respectively.\n",
    "\n",
    "# # For numerical columns, fill nulls with mean, median, or mode\n",
    "# for column in numeric_columns.columns:\n",
    "#     # Choose the imputation method - here, let's use mean for example\n",
    "#     numeric_columns[column].fillna(numeric_columns[column].mean(), inplace=True)\n",
    "#     # Alternatively, you could use median or mode:\n",
    "#     # numeric_columns[column].fillna(numeric_columns[column].median(), inplace=True)\n",
    "#     # numeric_columns[column].fillna(numeric_columns[column].mode()[0], inplace=True)\n",
    "\n",
    "# # For categorical columns, fill nulls with mode or create a new category 'Missing'\n",
    "# for column in object_columns.columns:\n",
    "#     # Fill with mode (most frequent value)\n",
    "#     object_columns[column].fillna(object_columns[column].mode()[0], inplace=True)\n",
    "#     # Alternatively, you could create a new category for missing values:\n",
    "#     # object_columns[column].fillna('Missing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder()\n",
    "for i in object_columns.select_dtypes(include=\"object\").columns:\n",
    "    object_columns[i]=enc.fit_transform(object_columns[i])\n",
    "\n",
    "object_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check columns where non-null count is greater than 4185 it is 50% of date is below columns are displayed.\n",
    "non_null_count = Final.notna().sum()\n",
    "columns_with_high_non_null = non_null_count[non_null_count < 4142]\n",
    "\n",
    "print(\"Columns with non-null count lesser than 4185:\")\n",
    "print(columns_with_high_non_null)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Z-score Analysis\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Calculate Z-scores for numerical features, excluding 'price'\n",
    "ri_no_price = ri[numerical_features]\n",
    "z_scores = zscore(ri_no_price)\n",
    "\n",
    "# Identify outliers (absolute Z-score greater than 3)\n",
    "ri_cleaned = ri[(z_scores < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Necessary Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Step 2: Define Features and Target Variable\n",
    "X = ri_cleaned.drop(columns=['price'])  # Drop the target column from the feature set\n",
    "y = ri_cleaned['price']  # Define the target variable\n",
    "\n",
    "print(ri_cleaned.shape)\n",
    "print(ri_cleaned.columns)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = ri.drop(['price'], axis=1)  # Features\n",
    "y = ri['price']  # Target variable\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print shapes of the splits\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "X = ri_cleaned.drop(columns=['price'], errors='ignore')  # Ignore error if 'price' is missing\n",
    "y = ri_cleaned['price'] if 'price' in ri_cleaned else None\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print shapes of the splits\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split the Dataset into Training and Testing Sets\n",
    "if X.shape[0] > 0 and y is not None:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  # 70-30 split\n",
    "else:\n",
    "    print(\"Error: No data available for training and testing.\")\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop('price', axis=1)  # Features\n",
    "y = df['price']               # Target variable\n",
    "\n",
    "# Perform the train-test split\n",
    "# Common split ratio: 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print shapes of the resulting datasets\n",
    "print(f\"Training set features shape: {X_train.shape}\")\n",
    "print(f\"Testing set features shape: {X_test.shape}\")\n",
    "print(f\"Training set target shape: {y_train.shape}\")\n",
    "print(f\"Testing set target shape: {y_test.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
